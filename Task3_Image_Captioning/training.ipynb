{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning Training Notebook\n",
    "\n",
    "This notebook demonstrates how to train the image captioning model on datasets like Flickr8k, Flickr30k, or COCO Captions.\n",
    "\n",
    "## Overview\n",
    "- Data loading and preprocessing\n",
    "- Model training setup\n",
    "- Training loop implementation\n",
    "- Evaluation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "For this demo, we'll create a mock dataset. In practice, you would use:\n",
    "- Flickr8k: 8,000 images with 5 captions each\n",
    "- Flickr30k: 31,000 images with 5 captions each\n",
    "- COCO Captions: 330,000 images with 5 captions each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for Image Captioning\"\"\"\n",
    "    \n",
    "    def __init__(self, image_dir, captions_file, transform=None, vocab=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load captions\n",
    "        with open(captions_file, 'r') as f:\n",
    "            self.captions = json.load(f)\n",
    "        \n",
    "        # Build vocabulary if not provided\n",
    "        if vocab is None:\n",
    "            self.vocab = self.build_vocab()\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        \n",
    "        # Image paths\n",
    "        self.image_paths = list(self.captions.keys())\n",
    "    \n",
    "    def build_vocab(self, threshold=5):\n",
    "        \"\"\"Build vocabulary from captions\"\"\"\n",
    "        counter = Counter()\n",
    "        \n",
    "        for captions in self.captions.values():\n",
    "            for caption in captions:\n",
    "                tokens = str(caption).lower().split()\n",
    "                counter.update(tokens)\n",
    "        \n",
    "        # Create vocabulary\n",
    "        vocab = {'<pad>': 0, '<start>': 1, '<end>': 2, '<unk>': 3}\n",
    "        \n",
    "        for word, count in counter.items():\n",
    "            if count >= threshold:\n",
    "                vocab[word] = len(vocab)\n",
    "        \n",
    "        return vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.image_dir, self.image_paths[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get random caption for this image\n",
    "        captions = self.captions[self.image_paths[idx]]\n",
    "        caption = np.random.choice(captions)\n",
    "        \n",
    "        # Convert caption to tokens\n",
    "        tokens = ['<start>'] + str(caption).lower().split() + ['<end>']\n",
    "        caption_ids = [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]\n",
    "        \n",
    "        return image, torch.tensor(caption_ids)\n",
    "\n",
    "# Create mock dataset for demonstration\n",
    "def create_mock_dataset(num_samples=100):\n",
    "    \"\"\"Create a mock dataset for demonstration\"\"\"\n",
    "    mock_captions = {}\n",
    "    \n",
    "    sample_captions = [\n",
    "        \"A dog is running in the park\",\n",
    "        \"A cat is sitting on the couch\",\n",
    "        \"People are walking on the beach\",\n",
    "        \"A car is driving on the road\",\n",
    "        \"Trees are standing in the forest\",\n",
    "        \"A bird is flying in the sky\",\n",
    "        \"Children are playing in the playground\",\n",
    "        \"A house is built on the hill\",\n",
    "        \"Flowers are blooming in the garden\",\n",
    "        \"A boat is sailing on the water\"\n",
    "    ]\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        img_name = f\"image_{i:04d}.jpg\"\n",
    "        # Each image has 5 captions\n",
    "        mock_captions[img_name] = np.random.choice(sample_captions, 5, replace=True).tolist()\n",
    "    \n",
    "    return mock_captions\n",
    "\n",
    "# Save mock dataset\n",
    "mock_data = create_mock_dataset(100)\n",
    "with open('mock_captions.json', 'w') as f:\n",
    "    json.dump(mock_data, f, indent=2)\n",
    "\n",
    "print(\"Mock dataset created with 100 images and 5 captions each\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transforms and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "dataset = ImageCaptionDataset(\n",
    "    image_dir='sample_images',  # You would use actual image directory\n",
    "    captions_file='mock_captions.json',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Vocabulary size: {len(dataset.vocab)}\")\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Create data loader\n",
    "def collate_fn(data):\n",
    "    \"\"\"Custom collate function for padding\"\"\"\n",
    "    images, captions = zip(*data)\n",
    "    \n",
    "    # Stack images\n",
    "    images = torch.stack(images, 0)\n",
    "    \n",
    "    # Pad captions\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    max_length = max(lengths)\n",
    "    \n",
    "    padded_captions = torch.zeros(len(captions), max_length).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        padded_captions[i, :len(cap)] = cap\n",
    "    \n",
    "    return images, padded_captions, lengths\n",
    "\n",
    "# Create DataLoader\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created with batch size 32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model components\n",
    "from model import ImageCaptioningModel\n",
    "\n",
    "# Initialize model\n",
    "model = ImageCaptioningModel(\n",
    "    encoder_type='resnet',\n",
    "    decoder_type='lstm',  # or 'transformer'\n",
    "    embed_size=256,\n",
    "    hidden_size=512,\n",
    "    vocab_size=len(dataset.vocab),\n",
    "    fine_tune_encoder=False\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab['<pad>'])\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 20\n",
    "print_every = 10\n",
    "save_every = 5\n",
    "\n",
    "print(\"Training setup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (images, captions, lengths) in enumerate(tqdm(data_loader)):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images, captions[:, :-1], lengths)\n",
    "        \n",
    "        # Calculate loss\n",
    "        # Reshape for cross entropy loss\n",
    "        batch_size, seq_len, vocab_size = outputs.shape\n",
    "        outputs = outputs.reshape(batch_size * seq_len, vocab_size)\n",
    "        targets = captions[:, 1:].reshape(batch_size * seq_len)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print progress\n",
    "        if batch_idx % print_every == 0:\n",
    "            print(f'Batch [{batch_idx}/{len(data_loader)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def validate(model, data_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, captions, lengths in data_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            outputs = model(images, captions[:, :-1], lengths)\n",
    "            \n",
    "            batch_size, seq_len, vocab_size = outputs.shape\n",
    "            outputs = outputs.reshape(batch_size * seq_len, vocab_size)\n",
    "            targets = captions[:, 1:].reshape(batch_size * seq_len)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"Note: This is a demonstration with mock data.\")\n",
    "print(\"For real training, use actual image datasets.\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch [{epoch+1}/{num_epochs}]')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, data_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(model, data_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    print(f'Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n",
    "    \n",
    "    # Save model checkpoint\n",
    "    if (epoch + 1) % save_every == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'vocab': dataset.vocab,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses\n",
    "        }\n",
    "        torch.save(checkpoint, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        print(f'Checkpoint saved: checkpoint_epoch_{epoch+1}.pth')\n",
    "\n",
    "print('\\nTraining completed!')\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab': dataset.vocab,\n",
    "    'config': {\n",
    "        'embed_size': 256,\n",
    "        'hidden_size': 512,\n",
    "        'decoder_type': 'lstm'\n",
    "    }\n",
    "}, 'final_model.pth')\n",
    "\n",
    "print('Final model saved: final_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training curves plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model for inference\n",
    "def load_model(checkpoint_path, vocab):\n",
    "    \"\"\"Load trained model from checkpoint\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model = ImageCaptioningModel(\n",
    "        encoder_type='resnet',\n",
    "        decoder_type='lstm',\n",
    "        embed_size=256,\n",
    "        hidden_size=512,\n",
    "        vocab_size=len(vocab),\n",
    "        fine_tune_encoder=False\n",
    "    ).to(device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Generate caption for an image\n",
    "def generate_caption(model, image_path, vocab, transform, max_length=20):\n",
    "    \"\"\"Generate caption for a single image\"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate caption\n",
    "    with torch.no_grad():\n",
    "        caption_ids = model.generate_caption(image_tensor, max_length)\n",
    "    \n",
    "    # Convert IDs to words\n",
    "    idx_to_word = {v: k for k, v in vocab.items()}\n",
    "    caption_words = []\n",
    "    \n",
    "    for idx in caption_ids:\n",
    "        if idx in idx_to_word:\n",
    "            word = idx_to_word[idx]\n",
    "            if word == '<end>':\n",
    "                break\n",
    "            elif word not in ['<start>', '<pad>', '<unk>']:\n",
    "                caption_words.append(word)\n",
    "    \n",
    "    return ' '.join(caption_words).capitalize() + '.'\n",
    "\n",
    "print(\"Inference functions defined\")\n",
    "print(\"Note: For actual inference, you need:\")\n",
    "print(\"1. Trained model checkpoint\")\n",
    "print(\"2. Real images in sample_images directory\")\n",
    "print(\"3. Vocabulary from training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU Score calculation (simplified)\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def bleu_score(reference, candidate, n=4):\n",
    "    \"\"\"Calculate BLEU score (simplified version)\"\"\"\n",
    "    def get_ngrams(tokens, n):\n",
    "        ngrams = []\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngrams.append(tuple(tokens[i:i+n]))\n",
    "        return Counter(ngrams)\n",
    "    \n",
    "    ref_tokens = reference.lower().split()\n",
    "    cand_tokens = candidate.lower().split()\n",
    "    \n",
    "    if len(cand_tokens) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precisions = []\n",
    "    \n",
    "    for i in range(1, n + 1):\n",
    "        ref_ngrams = get_ngrams(ref_tokens, i)\n",
    "        cand_ngrams = get_ngrams(cand_tokens, i)\n",
    "        \n",
    "        if not cand_ngrams:\n",
    "            precisions.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        # Count overlapping n-grams\n",
    "        overlap = 0\n",
    "        for ngram, count in cand_ngrams.items():\n",
    "            overlap += min(count, ref_ngrams.get(ngram, 0))\n",
    "        \n",
    "        total = sum(cand_ngrams.values())\n",
    "        precisions.append(overlap / total)\n",
    "    \n",
    "    # Brevity penalty\n",
    "    bp = 1.0\n",
    "    if len(cand_tokens) < len(ref_tokens):\n",
    "        bp = math.exp(1 - len(ref_tokens) / len(cand_tokens))\n",
    "    \n",
    "    # Geometric mean of precisions\n",
    "    if all(p > 0 for p in precisions):\n",
    "        score = bp * math.exp(sum(math.log(p) for p in precisions) / n)\n",
    "    else:\n",
    "        score = 0.0\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Example usage\n",
    "reference = \"a brown dog is running in the grass\"\n",
    "candidate = \"a dog runs on the grass\"\n",
    "\n",
    "bleu = bleu_score(reference, candidate)\n",
    "print(f\"BLEU Score: {bleu:.4f}\")\n",
    "\n",
    "print(\"\\nNote: For comprehensive evaluation, use:\")\n",
    "print(\"- NLTK for BLEU, METEOR\")\n",
    "print(\"- pycocoevalcap for CIDEr, SPICE\")\n",
    "print(\"- Multiple reference captions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### For Real Training:\n",
    "1. **Download Datasets**:\n",
    "   - Flickr8k: https://www.kaggle.com/datasets/adityajn105/flickr8k\n",
    "   - COCO Captions: https://cocodataset.org/#download\n",
    "\n",
    "2. **Data Preparation**:\n",
    "   - Extract images and captions\n",
    "   - Split into train/val/test sets\n",
    "   - Build proper vocabulary\n",
    "\n",
    "3. **Training Configuration**:\n",
    "   - Adjust hyperparameters\n",
    "   - Use GPU for faster training\n",
    "   - Implement early stopping\n",
    "\n",
    "4. **Model Improvements**:\n",
    "   - Add attention mechanisms\n",
    "   - Implement beam search\n",
    "   - Use transformer decoder\n",
    "   - Fine-tune CNN encoder\n",
    "\n",
    "5. **Evaluation**:\n",
    "   - Calculate BLEU, METEOR, CIDEr scores\n",
    "   - Visualize attention maps\n",
    "   - Test on diverse images\n",
    "\n",
    "### Deployment Options:\n",
    "- Web application with Flask/FastAPI\n",
    "- Mobile app with TensorFlow Lite\n",
    "- Cloud service deployment\n",
    "- Edge device optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
